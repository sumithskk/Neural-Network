{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook recreates neural network algorithms from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the csv files from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath,names = [i for i in range(1,786)]) #read the csv files from path\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    df_N=df.drop([785],axis=1)\n",
    "    X=df_N.to_numpy()\n",
    "    Y=df[785].to_numpy()\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX,YY = load_data('F:/Data Science/NN/mnist_train.csv')\n",
    "# Training Samples\n",
    "X=XX[:8000];Y=YY[:8000]\n",
    "# Test Samples\n",
    "X_test = XX[8000:];Y_test = YY[8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function will give two arrays, one is pixel values and other target values.\n",
    "I splitted the array in to two, first 8000 for training my neural network and last 2000 for testing my neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax Activation Function\n",
    "def softmax(X):\n",
    "    expX = np.exp(X-np.max(X,axis=1,keepdims=True))\n",
    "    return expX / np.sum(expX, axis=1, keepdims=True) \n",
    "\n",
    "#Sigmoid Activation Function\n",
    "def sigmoid(X): \n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "#Relu Activation Function\n",
    "def relu(X):\n",
    "    X[X<=0]=0\n",
    "    return X\n",
    "\n",
    "#Derivative of Relu Activation Function\n",
    "def relu_der(x):\n",
    "    x[x<=0]=0\n",
    "    x[x>0]=1 \n",
    "    return x\n",
    "\n",
    "\n",
    "#Tanh Activation Function\n",
    "def tanh(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "#Derivative of Sigmoid Activation Function\n",
    "def sigmoid_der(X):\n",
    "    return sigmoid(X)*(1-sigmoid(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network algorithm with  zero hidden layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_layer         = 784       # Input Layer\n",
    "out_layer        = 10        # Output Layer\n",
    "lr_zero          = 1         # Learning Rate\n",
    "iteration_zero   = 1000      # No of Iterations\n",
    "num_examples     = len(X)    # No of Input training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Weights and Bias(Using Random)\n",
    "def weight_bias_zero_layer(in_layer,out_layer):\n",
    "    w=np.random.random((in_layer,out_layer))\n",
    "    b=np.zeros((1,out_layer)) \n",
    "    return (w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating Weights and Bias \n",
    "def update_weights_perceptron(X, Y, weights_zero, bias_zero, lr_zero):\n",
    "    X = (X/np.max(X))  # Normalize the output with max as zero for better numerical stability of exponential\n",
    "    w = weights_zero #(784,8000)  #Initial Weights which we generated randomly\n",
    "    b = bias_zero    #(1,10)      #Initial bias which an array of zeros\n",
    "    print(\"Neural Network Algorithm  Zero Hidden Layers\")\n",
    "    print(\"Epochs : {}  Learning Rate : {}\".format(iteration_zero,lr_zero))\n",
    "    print(\"Weights and Bias are updating .....\")\n",
    "    print(\"   Loss (Cross-Entropy)\")\n",
    "    for i in range(iteration_zero):\n",
    "        #Forward pass:\n",
    "            probs = softmax(np.dot(X,w)+b)    # Apply Softmax activation\n",
    "            #Probs will be a array of dimension (1,out_layer) \n",
    "        #Loss calculation:\n",
    "            cross_er = -np.log(probs[np.arange(num_examples), Y]) #Cross-Entropy\n",
    "            loss = np.sum(cross_er)/num_examples        #Average loss in the training\n",
    "            if(i%200==0):\n",
    "                print(\"\\t {}th Iteration: {}\".format(i,loss))\n",
    "        #backward pass:\n",
    "            delta = probs.copy()\n",
    "            delta[range(num_examples), Y] -= 1\n",
    "            delta/=num_examples\n",
    "            \n",
    "            dw = np.dot(X.T,delta)\n",
    "            db = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "            w += -lr_zero*dw #Updated Weights \n",
    "            b += -lr_zero*db #Updated bias\n",
    "    print(\"Final train Loss {}\".format(loss))\n",
    "    updated_weights= w\n",
    "    updated_bias   = b\n",
    "    return (updated_weights, updated_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Algorithm  Zero Hidden Layers\n",
      "Epochs : 1000  Learning Rate : 1\n",
      "Weights and Bias are updating .....\n",
      "   Loss (Cross-Entropy)\n",
      "\t 0th Iteration: 4.450610328460305\n",
      "\t 200th Iteration: 0.28238707433409266\n",
      "\t 400th Iteration: 0.2370356923769022\n",
      "\t 600th Iteration: 0.21354490585689706\n",
      "\t 800th Iteration: 0.19768873890747807\n",
      "Final train Loss 0.1857768211566185\n"
     ]
    }
   ],
   "source": [
    "#Initial Weights and Bias\n",
    "weights_zero, bias_zero = weight_bias_zero_layer(in_layer,out_layer)\n",
    "#Updating Weights and bias\n",
    "weights_zero_updated,bias_zero_updated =  update_weights_perceptron(X, Y, weights_zero, bias_zero, lr_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Y : 0\n",
      "Target\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing the Neural Network using last 2000 samples\n",
    "def predict_perceptron(Q,weights_zero,bias_zero):\n",
    "    Q = (Q/255.0)\n",
    "    w = weights_zero\n",
    "    b = bias_zero\n",
    "    z=np.dot(Q,w)+b\n",
    "    predicted = softmax(z/np.max(z))\n",
    "    print(\"Predicted Y : {}\" .format(np.argmax(predicted)))\n",
    "    print(\"Target\")\n",
    "    plt.imshow(np.array(Q.reshape(28,28)),cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "predict_perceptron(X_test[147],weights_zero_updated,bias_zero_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly labeled (Out of 2000 Samples) : 1799\n",
      " Accuracy : 89.95 \n"
     ]
    }
   ],
   "source": [
    "#Accuracy of Neural Network using last 2000 samples\n",
    "def accuracy_perceptron(Q,weights_zero,bias_zero):\n",
    "    Q = (Q/255.0)\n",
    "    w = weights_zero\n",
    "    b = bias_zero\n",
    "    z=np.dot(Q,w)+b\n",
    "    predicted = softmax(z/np.max(z)) \n",
    "    Y_p = np.argmax(predicted,axis=1)\n",
    "    correct = 0\n",
    "    for i in range(len(Q)):\n",
    "        if Y_p[i]==Y_test[i]:\n",
    "            correct =correct+1\n",
    "        else:\n",
    "            continue\n",
    "    print(\"Correctly labeled (Out of {} Samples) : {}\".format(len(X_test),correct))  \n",
    "    print(\" Accuracy : {} \".format(correct/len(Q)*100))\n",
    "accuracy_perceptron(X_test,weights_zero,bias_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Algorithm One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_layer         = 784     # Input Layer\n",
    "hid_layer        = 10      # Hidden Layer\n",
    "out_layer        = 10      # Output Layer\n",
    "lr_single        = 5     # Learning Rate\n",
    "iteration_single = 1000    # No of Iterations\n",
    "num_examples     = len(X)  # No of Input training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Weights and Bias\n",
    "def weight_bias_single_layer(in_layer,hid_layer,out_layer):\n",
    "        w1=np.random.randn(in_layer,hid_layer)*0.01   #weights 1 (784,10)\n",
    "        w2=np.random.randn(hid_layer,out_layer)*0.01  #weights 2 (10 , 10)\n",
    "        weights = [w1,w2]\n",
    "        b1=np.zeros((1,hid_layer))  #bias 1  (1,10)\n",
    "        b2=np.zeros((1,out_layer))  #bias 2  (1,10)\n",
    "        bias = [b1,b2]\n",
    "        return (weights,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Updating Weights and Bias \n",
    "def update_weights_single_layer(X, Y, weights_single, bias_single, lr_single):\n",
    "    X= X/255.0   # Normalize the output with max as zero for better numerical stability of exponential\n",
    "    [w1,w2] = weights_single #Initial Weights \n",
    "    [b1,b2] = bias_single    #Initial Bias\n",
    "    print(\"Neural Network Algorithm Single Hidden Layer\")\n",
    "    print(\"Epochs : {}  Learning Rate : {}\".format(iteration_single,lr_single))\n",
    "    print(\"Weights and Bias ara updating .....\")\n",
    "    print(\"   Loss (Cross-Entropy)\")\n",
    "    for i in range(iteration_single):\n",
    "    #Forward pass:\n",
    "        z1 = np.dot(X,w1) + b1        \n",
    "        A1 = sigmoid(z1)        # Apply sigmoid activation\n",
    "        z2 = np.dot(A1,w2) + b2\n",
    "        probs = softmax(z2)     # Apply Softmax activation\n",
    "        \n",
    "    #Loss calculation:\n",
    "        cross_er = -np.log(probs[range(num_examples), Y])\n",
    "        loss = np.sum(cross_er)/num_examples   \n",
    "        \n",
    "        if(i%200==0):\n",
    "            print(\"\\t {}th Iteration: {}\".format(i,loss))\n",
    "        \n",
    "    #backward pass:\n",
    "        delta = probs\n",
    "        delta[range(num_examples), Y] -= 1\n",
    "        delta/=num_examples\n",
    "\n",
    "        dw2 =np.dot(A1.T,delta)\n",
    "        db2 = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "        dA1 = np.dot(probs,w2.T)\n",
    "        dz1 = dA1*sigmoid(z1)*(1-sigmoid(z1))\n",
    "        dw1 =np.dot(X.T,dz1)\n",
    "        db1 =np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        w2 += -lr_single*dw2  #Upadate weight 1\n",
    "        b2 += -lr_single*db2  #Upadate bias 1\n",
    "        w1 += -lr_single*dw1  #Upadate weight 2\n",
    "        b1 += -lr_single*db1  #Upadate bias 2\n",
    "        \n",
    "    print(\"Final train Loss {}\".format(loss))\n",
    "    updated_weights = [w1,w2]\n",
    "    updated_bias    = [b1,b2]\n",
    "    return (updated_weights,updated_bias) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Algorithm Single Hidden Layer\n",
      "Epochs : 1000  Learning Rate : 5\n",
      "Weights and Bias ara updating .....\n",
      "   Loss (Cross-Entropy)\n",
      "\t 0th Iteration: 2.3030192073119706\n",
      "\t 200th Iteration: 0.20844752001612918\n",
      "\t 400th Iteration: 0.14980806230178584\n",
      "\t 600th Iteration: 0.18866395897006072\n",
      "\t 800th Iteration: 0.10158018020498936\n",
      "Final train Loss 0.08581081153910483\n"
     ]
    }
   ],
   "source": [
    "#Initial Weights and Bias\n",
    "weights_single, bias_single = weight_bias_single_layer(in_layer,hid_layer,out_layer)\n",
    "#Updating Weights and bias\n",
    "weights_single_up,bias_single_up= update_weights_single_layer(X, Y, weights_single, bias_single, lr_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 8\n",
      "Target\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing the Neural Network using last 2000 samples\n",
    "def predict_single_layer(Q,weights_single_up,bias_single_up):\n",
    "    [w1,w2] = weights_single_up\n",
    "    [b1,b2] = bias_single_up\n",
    "    Q = (Q/255.0)\n",
    "    z1 = np.dot(Q,w1) + b1 # Apply sigmoid activation\n",
    "    A1 = sigmoid(z1)\n",
    "    z2 = np.dot(A1,w2) + b2\n",
    "    predicted = softmax(z2)\n",
    "    print(\"Predicted: {}\" .format(np.argmax(predicted)))\n",
    "    print(\"Target\")\n",
    "    plt.imshow(np.array(Q.reshape(28,28)),cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "predict_single_layer(X_test[132],weights_single_up,bias_single_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly labeled (Out of 2000 Samples) : 1812\n",
      " Accuracy : 90.60000000000001 \n"
     ]
    }
   ],
   "source": [
    "#Accuracy of Neural Network using last 2000 samples\n",
    "def accuracy_single_layer(Q,weights_single,bias_single):\n",
    "    Q = (Q/255.0)\n",
    "    [w1,w2] = weights_single\n",
    "    [b1,b2] = bias_single\n",
    "    z1 = np.dot(Q,w1) + b1\n",
    "    A1 = sigmoid(z1)\n",
    "    z2 = np.dot(A1,w2) + b2\n",
    "    predicted = softmax(z2)\n",
    "    Y_p = np.argmax(predicted,axis=1)\n",
    "    correct = 0\n",
    "    for i in range(len(Q)):\n",
    "        if Y_p[i]==Y_test[i]:\n",
    "            correct =correct+1\n",
    "        else:\n",
    "            continue\n",
    "    print(\"Correctly labeled (Out of {} Samples) : {}\".format(len(X_test),correct))\n",
    "    print(\" Accuracy : {} \".format(correct/len(Q)*100))\n",
    "accuracy_single_layer(X_test,weights_single,bias_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Algorithm Two Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_layer         = 784        # Input Layer\n",
    "hid_layer_1      = 10         # First Hidden Layer\n",
    "hid_layer_2      = 10         # Second Hidden Layer\n",
    "out_layer        = 10         # Output Layer\n",
    "lr_double        = 5          # Learning Rate\n",
    "iteration_double = 1000       # No of Iterations\n",
    "num_examples = len(X)         # No of Input training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Weights and Bias\n",
    "def weight_bias_double_layer(in_layer,hid_layer_1,hid_layer_2,out_layer):\n",
    "        w1=np.random.randn(in_layer,hid_layer_1)*0.01     #weights 1 (784,10)\n",
    "        w2=np.random.randn(hid_layer_1,hid_layer_2)*0.01  #weights 2 (10 , 10)\n",
    "        w3=np.random.randn(hid_layer_2,out_layer)*0.01    #weights 3 (10 , 10)\n",
    "        weights = [w1,w2,w3]\n",
    "        b1=np.zeros((1,hid_layer_1))  #bias 1 (1,10)\n",
    "        b2=np.zeros((1,hid_layer_2))  #bias 2 (1,10)\n",
    "        b3=np.zeros((1,out_layer))    #bias 3 (1,10)\n",
    "        bias = [b1,b2,b3]\n",
    "        return (weights,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Updating Weights and Bias\n",
    "def update_weights_double_layer(X, Y,weights_double,bias_double, lr_double):\n",
    "    X= X/255.0    # Normalize the output with max as zero for better numerical stability of exponential\n",
    "    [w1,w2,w3] = weights_double  #Input Weights\n",
    "    [b1,b2,b3] = bias_double     #Input Bias\n",
    "    print(\"Neural Network Algorithm  Two Hidden Layers\")\n",
    "    print(\"Epochs : {}  Learning Rate : {}\".format(iteration_double,lr_double))\n",
    "    print(\"Weights and Bias ara updating .....\")\n",
    "    print(\"   Loss (Cross-Entropy)\")\n",
    "    for i in range(iteration_double):\n",
    "    #Forward pass:\n",
    "        z1 = np.dot(X,w1) + b1   # Input Layer-- Hidden layer 1\n",
    "        A1 = sigmoid(z1)         #Apply Sigmoid Activation\n",
    "        z2 = np.dot(A1,w2) + b2  # Hidden layer 1-- Hidden layer 2\n",
    "        A2 = sigmoid(z2)         #Apply Sigmoid Activation\n",
    "        z3 = np.dot(A2,w3) + b3  # Hidden layer 2-- Out Layer\n",
    "        probs = softmax(z3)      #Apply softmax Activation\n",
    "        \n",
    "    #Loss calculation:(Cross Entropy)\n",
    "        cross_er = -np.log(probs[range(num_examples), Y])\n",
    "        loss = np.sum(cross_er)/num_examples  #Average loss\n",
    "        \n",
    "        if(i%200==0):\n",
    "            print(\"\\t {}th Iteration: {}\".format(i,loss))\n",
    "            \n",
    "    #Back Propagation\n",
    "        delta = probs\n",
    "        delta[range(num_examples), Y] -= 1\n",
    "        delta/=num_examples\n",
    "       \n",
    "        dw3 =np.dot(A2.T,delta)\n",
    "        db3 = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "        dA2 = np.dot(probs,w3.T)\n",
    "        dz2 = dA2*sigmoid(z2)*(1-sigmoid(z2))\n",
    "        dw2 =np.dot(A1.T,dz2)\n",
    "        db2 =np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        dA1 = np.dot(probs,w3.T)\n",
    "        dz1 = dA1*sigmoid(z2)*(1-sigmoid(z2)) \n",
    "        dz11 = np.dot(dz1,w2.T)\n",
    "        dz12 = dz11*sigmoid(z1)*(1-sigmoid(z1))\n",
    "        dw1 =np.dot(X.T,dz12)\n",
    "        db1 =np.sum(dz12, axis=0, keepdims=True)\n",
    "        \n",
    "        w3 += -lr_double*dw3  #Upadate weight 3\n",
    "        b3 += -lr_double*db3  #Upadate bias 3\n",
    "        w2 += -lr_double*dw2  #Upadate weight 2\n",
    "        b2 += -lr_double*db2  #Upadate bias 2\n",
    "        w1 += -lr_double*dw1  #Upadate weight 1\n",
    "        b1 += -lr_double*db1  #Upadate bias 1\n",
    "        \n",
    "    print(\" Final train Loss {}\".format(loss))\n",
    "    updated_weights = [w1,w2,w3]  #Updated Weights\n",
    "    updated_bias    = [b1,b2,b3]  #Updated Bias\n",
    "    return (updated_weights,updated_bias) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Algorithm  Two Hidden Layers\n",
      "Epochs : 1000  Learning Rate : 5\n",
      "Weights and Bias ara updating .....\n",
      "   Loss (Cross-Entropy)\n",
      "\t 0th Iteration: 2.3021626484327724\n",
      "\t 200th Iteration: 2.2195198826277465\n",
      "\t 400th Iteration: 0.6982211710364956\n",
      "\t 600th Iteration: 0.3875543066252159\n",
      "\t 800th Iteration: 0.2003410746140178\n",
      " Final train Loss 0.19266791745936512\n"
     ]
    }
   ],
   "source": [
    "#Initial Weights and Bias\n",
    "weights_double, bias_double = weight_bias_double_layer(in_layer,hid_layer_1,hid_layer_2,out_layer)\n",
    "#Updating Weights and bias\n",
    "updated_weights_double,updated_bias_double = update_weights_double_layer(X, Y,weights_double,bias_double, lr_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Y : 4\n",
      "Target Y :\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAEZklEQVR4nO3dUU7bQBRA0brqvpKsLLCywMrcDSSeFmN8nZzzyQiIEq5G4mnG0zzPv4Ce33u/AOA+cUKUOCFKnBAlToj6s7Q4TZN/5cLG5nme7n3dzglR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6I+rP3Cyg6n8+r1t/e3r7ttfC67JwQJU6IEidEiROixAlR4oSoaZ7nx4vT9HjxwEajjtPptLg+GqWMTNO06vuf1dL7OnrPPz4+Vq3vaZ7nu38Qdk6IEidEiROixAlR4oQocUKUOCHqaeecS3Ox2+32cy/kjsvl8nCtPI/b2tL8+Xq9rvrZ7+/vX/7dWzPnhIMRJ0SJE6LECVHihChxQpQ4IeqwV2OOzvetnYstGc0iRzO1V51ljubLa8/JLhmd0S2yc0KUOCFKnBAlTogSJ0SJE6LECVFPO+fccmZmjnnfnp/JyOfn526/+6vsnBAlTogSJ0SJE6LECVHihChxQtRh761det1rle843dNoTrnnfcBH/szcWwsHI06IEidEiROixAlR4oSo7JGxPY98lf/tvrWl933L60ZHXvEzs3NClDghSpwQJU6IEidEiROixAlRLznn5LGl933Pz+QZ55gjdk6IEidEiROixAlR4oQocUKUOCEqezXmlldfjh7RN3pc3NqZ254zuz3PZI5cLpeHa8/8WEVXY8LBiBOixAlR4oQocUKUOCFKnBCVnXOOjB435zxozyvePfsvzDnhYMQJUeKEKHFClDghSpwQJU6IOuycczTHXFo/nU6rfvZaS2cTR2dJR6999P17nuecprvjvJdnzgkHI06IEidEiROixAlR4oSo7CMAR0ZXJT7zVYpL9jx2NToSxv+xc0KUOCFKnBAlTogSJ0SJE6LECVGHnXPS86qz5a3YOSFKnBAlTogSJ0SJE6LECVHihChzzoMZndfc8upLZ2h/lp0TosQJUeKEKHFClDghSpwQJU6IMuc8mNEjALc0erwg38vOCVHihChxQpQ4IUqcECVOiDJKOZjROON8Pq/6+UvHvvZ8vOArsnNClDghSpwQJU6IEidEiROixAlR0zzPjxen6fEiu1j6vL7D5XJ5uObqy23M8zzd+7qdE6LECVHihChxQpQ4IUqcECVOiDLnPJjb7ba4vuV5zqUZKF9nzgkHI06IEidEiROixAlR4oQocUKUOefBjOaYo/Xr9bq47jznzzPnhIMRJ0SJE6LECVHihChxQpQ4IcqcE3ZmzgkHI06IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqMWrMYH92DkhSpwQJU6IEidEiROixAlRfwH+5OV7XX/sYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Predicting a Output from X_test\n",
    "def predict_double_layer(Q,weights,bias):\n",
    "    [w1,w2,w3] = weights   \n",
    "    [b1,b2,b3] = bias\n",
    "    Q  = (Q/255.0)\n",
    "    A1 = sigmoid(np.dot(Q,w1) + b1)\n",
    "    A2 = sigmoid(np.dot(A1,w2) + b2)\n",
    "    predicted = softmax(np.dot(A2,w3) + b3)\n",
    "    print(\"Predicted Y : {}\" .format(np.argmax(predicted)))\n",
    "    print(\"Target Y :\")\n",
    "    plt.imshow(np.array(Q.reshape(28,28)),cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "predict_double_layer(X_test[11],updated_weights_double,updated_bias_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly labeled (Out of 2000 Samples) : 1799\n",
      " Accuracy : 89.95 \n"
     ]
    }
   ],
   "source": [
    "#Accuracy of Neural Network using last 2000 samples\n",
    "def accuracy_double_layer(Q,weights,bias):\n",
    "    Q = (Q/255.0)\n",
    "    [w1,w2,w3] = weights \n",
    "    [b1,b2,b3] = bias\n",
    "    A1 = sigmoid(np.dot(Q,w1) + b1)\n",
    "    A2 = sigmoid(np.dot(A1,w2) + b2)\n",
    "    predicted = softmax(np.dot(A2,w3) + b3)    \n",
    "    Y_p = np.argmax(predicted,axis=1)\n",
    "    correct = 0\n",
    "    for i in range(len(Q)):\n",
    "        if Y_p[i]==Y_test[i]:\n",
    "            correct =correct+1\n",
    "        else:\n",
    "            continue\n",
    "    print(\"Correctly labeled (Out of {} Samples) : {}\".format(len(X_test),correct))  \n",
    "    print(\" Accuracy : {} \".format(correct/len(Q)*100))\n",
    "accuracy_double_layer(X_test,updated_weights_double,updated_bias_double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network different activations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def activate(activation,X):\n",
    "    if activation == 'sigmoid':\n",
    "        return sigmoid(X)\n",
    "    if activation == 'tanh':\n",
    "        return tanh(X)\n",
    "    if activation == 'relu':\n",
    "        return relu(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derivative of Active Functions\n",
    "def der_active(activation,X):\n",
    "    if activation == 'sigmoid':\n",
    "        return sigmoid(X)*(1-sigmoid(X))\n",
    "    if activation == 'tanh':\n",
    "        return 1-(tanh(X))**2\n",
    "    if activation == 'relu':\n",
    "        return relu_der(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Updating Weights and Bias\n",
    "def update_weights_double_layer_act(X, Y,weights,bias,lr_act, activation):\n",
    "    X= X/255.0   # Normalize the output with max as zero for better numerical stability of exponential\n",
    "    [w1,w2,w3] = weights #Input Weights\n",
    "    [b1,b2,b3] = bias    #Input bias\n",
    "    print(\"Activation Function : {}\".format(activation))\n",
    "    print(\"Epochs : {}  Learning Rate : {}\".format(iteration_act,lr_act))\n",
    "    print(\"Weights and Bias are updating .....\")\n",
    "    for i in range(iteration_act):\n",
    "    #Forward pass:\n",
    "        z1 = np.dot(X,w1) + b1       # Input Layer-- Hidden layer 1\n",
    "        A1 = activate(activation,z1) #Apply Activation (Sigmoid,Relu,Tanh)\n",
    "        z2 = np.dot(A1,w2) + b2      # Hidden layer 1-- Hidden layer 2\n",
    "        A2 = activate(activation,z2) #Apply Activation (Sigmoid,Relu,Tanh)\n",
    "        z3 = np.dot(A2,w3) + b3      # Hidden layer 2-- Out Layer\n",
    "        probs = softmax(z3)          #Apply softmax Activation\n",
    "        \n",
    "    #Loss calculation:(Cross Entropy)\n",
    "        cross_er = -np.log(probs[range(num_examples), Y])\n",
    "        loss = np.sum(cross_er)/num_examples   #Average Entropy\n",
    "        \n",
    "    #backward propagation\n",
    "        delta = probs\n",
    "        delta[range(num_examples), Y] -= 1\n",
    "        delta/=num_examples\n",
    "\n",
    "        dw3 =np.dot(A2.T,delta)\n",
    "        db3 = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "        dA2 = np.dot(probs,w3.T)\n",
    "        dz2 = dA2*der_active(activation,sigmoid(z2))\n",
    "        dw2 = np.dot(A1.T,dz2)\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        dA1  = np.dot(probs,w3.T)\n",
    "        dz1  = dA1*der_active(activation,z2) \n",
    "        dz11 = np.dot(dz1,w2.T)\n",
    "        dz12 = dz11*der_active(activation,z1)\n",
    "        dw1  = np.dot(X.T,dz12)\n",
    "        db1  = np.sum(dz12, axis=0, keepdims=True)\n",
    "        \n",
    "        w3 += -lr_act*dw3 #Upadate weight 3\n",
    "        b3 += -lr_act*db3 #Upadate bias 3\n",
    "        w2 += -lr_act*dw2 #Upadate weight 2\n",
    "        b2 += -lr_act*db2 #Upadate bias 2\n",
    "        w1 += -lr_act*dw1 #Upadate weight 3\n",
    "        b1 += -lr_act*db1 #Upadate bias 1\n",
    "    \n",
    "        \n",
    "    print(\"  Final train Loss {}\".format(loss))\n",
    "    updated_weights = [w1,w2,w3]\n",
    "    updated_bias    = [b1,b2,b3]\n",
    "    return (updated_weights,updated_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of Neural Network using last 2000 samples\n",
    "def accuracy_act(Q,updated_weights,updated_bias,activation):\n",
    "    Q = (Q/255.0)\n",
    "    [w1,w2,w3] = updated_weights\n",
    "    [b1,b2,b3] = updated_bias\n",
    "    A1 = activate(activation,np.dot(Q,w1) + b1)\n",
    "    A2 = activate(activation,np.dot(A1,w2) + b2)\n",
    "    predicted = softmax(np.dot(A2,w3) + b3)    \n",
    "    Y_p = np.argmax(predicted,axis=1)\n",
    "    correct = 0\n",
    "    for i in range(len(Q)):\n",
    "        if Y_p[i]==Y_test[i]:\n",
    "            correct =correct+1\n",
    "        else:\n",
    "            continue\n",
    "    print(\"Correctly labeled (Out of {} Samples) : {}\".format(len(X_test),correct))  \n",
    "    print(\" Accuracy : {} \".format(correct/len(Q)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_act           = 3          # Learning Rate\n",
    "iteration_act    = 2000       # No of Iterations\n",
    "activation       = \"sigmoid\"  #Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function : sigmoid\n",
      "Epochs : 2000  Learning Rate : 3\n",
      "Weights and Bias are updating .....\n",
      "  Final train Loss 0.2625568497901256\n"
     ]
    }
   ],
   "source": [
    "#Initial Weights and Bias\n",
    "weights_double, bias_double = weight_bias_double_layer(in_layer,hid_layer_1,hid_layer_2,out_layer)\n",
    "\n",
    "#Updating Weights and Bias\n",
    "updated_weights,updated_bias = update_weights_double_layer_act(X, Y,weights_double, bias_double,lr_act, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_act           = 1          # Learning Rate\n",
    "iteration_act    = 1000       # No of Iterations\n",
    "activation       = \"tanh\"     # Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function : tanh\n",
      "Epochs : 1000  Learning Rate : 1\n",
      "Weights and Bias are updating .....\n",
      "  Final train Loss 0.22832755138374364\n"
     ]
    }
   ],
   "source": [
    "#Initial Weights and Bias\n",
    "weights_double, bias_double = weight_bias_double_layer(in_layer,hid_layer_1,hid_layer_2,out_layer)\n",
    "\n",
    "#Updating Weights and Bias\n",
    "updated_weights,updated_bias = update_weights_double_layer_act(X, Y,weights_double, bias_double,lr_act, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly labeled (Out of 2000 Samples) : 1727\n",
      " Accuracy : 86.35000000000001 \n"
     ]
    }
   ],
   "source": [
    "#Accuracy of Neural Network Using Last 2000 samples\n",
    "accuracy_act(X_test,updated_weights,updated_bias,activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_act           = 1          # Learning Rate\n",
    "iteration_act    = 3000      # No of Iterations\n",
    "activation       = \"relu\"     # Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function : relu\n",
      "Epochs : 3000  Learning Rate : 1\n",
      "Weights and Bias are updating .....\n",
      "  Final train Loss 2.3005448485528506\n"
     ]
    }
   ],
   "source": [
    "#Initial Weights and Bias\n",
    "weights_double, bias_double = weight_bias_double_layer(in_layer,hid_layer_1,hid_layer_2,out_layer)\n",
    "\n",
    "#Updating Weights and Bias\n",
    "updated_weights,updated_bias = update_weights_double_layer_act(X, Y,weights_double, bias_double,lr_act, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_double_layer_momentum(X, Y,weights,bias,lr, activation):\n",
    "    X= X/255.0\n",
    "    [w1,w2,w3] = weights\n",
    "    [b1,b2,b3] = bias\n",
    "    \n",
    "    #Initializing momentum terms\n",
    "    vel_w1=np.zeros((in_layer,hid_layer_1))\n",
    "    vel_w2=np.zeros((hid_layer_1,hid_layer_2))\n",
    "    vel_w3=np.zeros((hid_layer_2,out_layer))\n",
    "    vel_b1=np.zeros((1,hid_layer_1))\n",
    "    vel_b2=np.zeros((1,hid_layer_2))\n",
    "    vel_b3=np.zeros((1,out_layer))\n",
    "    beta = 0.9 \n",
    "    print(\"Gradient Descent With Moementum\")\n",
    "    print(\"Activation Function : {}\".format(activation))\n",
    "    print(\"Epochs : {}  Learning Rate : {}\".format(iteration_act,lr_act))\n",
    "    print(\"Weights and Bias are updating .....\")\n",
    "    for i in range(iteration_act):\n",
    "    #Forward pass:\n",
    "        z1 = np.dot(X,w1) + b1       # Input Layer-- Hidden layer 1\n",
    "        A1 = activate(activation,z1) #Apply Activation (Sigmoid,Relu,Tanh)\n",
    "        z2 = np.dot(A1,w2) + b2      # Hidden layer 1-- Hidden layer 2\n",
    "        A2 = activate(activation,z2) #Apply Activation (Sigmoid,Relu,Tanh)\n",
    "        z3 = np.dot(A2,w3) + b3      # Hidden layer 2-- Out Layer\n",
    "        probs = softmax(z3)          #Apply softmax Activation\n",
    "        \n",
    "    #Loss calculation:(Cross  Entroy)\n",
    "        cross_er = -np.log(probs[range(num_examples), Y])\n",
    "        loss = np.sum(cross_er)/num_examples   #Average loss\n",
    "\n",
    "    #backward propagation\n",
    "        delta = probs\n",
    "        delta[range(num_examples), Y] -= 1\n",
    "        delta/=num_examples\n",
    "\n",
    "        dw3 =np.dot(A2.T,delta)\n",
    "        db3 = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "        dA2 = np.dot(probs,w3.T)\n",
    "        dz2 = dA2*der_active(activation,sigmoid(z2))\n",
    "        dw2 = np.dot(A1.T,dz2)\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        dA1  = np.dot(probs,w3.T)\n",
    "        dz1  = dA1*der_active(activation,z2) \n",
    "        dz11 = np.dot(dz1,w2.T)\n",
    "        dz12 = dz11*der_active(activation,z1)\n",
    "        dw1  = np.dot(X.T,dz12)\n",
    "        db1  = np.sum(dz12, axis=0, keepdims=True)\n",
    "        \n",
    "     #Momentum terms\n",
    "        vel_w1  =  vel_w1*beta+(1-beta)*dw1  \n",
    "        vel_w2  =  vel_w2*beta+(1-beta)*dw2 \n",
    "        vel_w3  =  vel_w3*beta+(1-beta)*dw3 \n",
    "        \n",
    "        vel_b1  =  vel_b1*beta + (1-beta)*db1\n",
    "        vel_b2  =  vel_b2*beta + (1-beta)*db2\n",
    "        vel_b3  =  vel_b3*beta + (1-beta)*db3\n",
    "        \n",
    "        w3 += -lr_act*vel_w3 #Upadate weight 3 using momentum\n",
    "        b3 += -lr_act*vel_b3 #Upadate bias 3 using momentum\n",
    "        w2 += -lr_act*vel_w2 #Upadate weight 2 using momentum\n",
    "        b2 += -lr_act*vel_b2 #Upadate bias 2 using momentum\n",
    "        w1 += -lr_act*vel_w1 #Upadate weight 1 using momentum\n",
    "        b1 += -lr_act*vel_b1 #Upadate bias 1 using momentum\n",
    "\n",
    "        \n",
    "    print(\"Final train Loss {}\".format(loss))\n",
    "    updated_weights = [w1,w2,w3]  #Updated Weights\n",
    "    updated_bias    = [b1,b2,b3]  #Updated Bias\n",
    "    return (updated_weights,updated_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_act           = 3          # Learning Rate\n",
    "iteration_act    = 1000       # No of Iterations\n",
    "activation       = \"sigmoid\"  #Activattion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent With Moementum\n",
      "Activation Function : sigmoid\n",
      "Epochs : 1000  Learning Rate : 3\n",
      "Weights and Bias are updating .....\n",
      "Final train Loss 0.25901309251489746\n"
     ]
    }
   ],
   "source": [
    "#Initial Weights and Bias\n",
    "weights_act,bias_act = weight_bias_double_layer(in_layer,hid_layer_1,hid_layer_2,out_layer)\n",
    "\n",
    "#Updating Weights and Bias\n",
    "updated_weights,updated_bias = update_weights_double_layer_momentum(X, Y,weights_act,bias_act,lr_act, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly labeled (Out of 2000 Samples) : 1709\n",
      " Accuracy : 85.45 \n"
     ]
    }
   ],
   "source": [
    "#Accuracy of Neural Network Using Last 2000 samples\n",
    "accuracy_act(X_test,updated_weights,updated_bias,activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_act           = 1          # Learning Rate\n",
    "iteration_act    = 1000       # No of Iterations\n",
    "activation       = \"tanh\"     #Activattion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent With Moementum\n",
      "Activation Function : tanh\n",
      "Epochs : 1000  Learning Rate : 1\n",
      "Weights and Bias are updating .....\n",
      "Final train Loss 0.06846647300691011\n"
     ]
    }
   ],
   "source": [
    "#Initial Weight and Bias\n",
    "weights_act, bias_act = weight_bias_double_layer(in_layer,hid_layer_1,hid_layer_2,out_layer)\n",
    "\n",
    "#Updating Weights and Bias\n",
    "updated_weights,updated_bias = update_weights_double_layer_momentum(X, Y,weights_act,bias_act,lr_act, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly labeled (Out of 2000 Samples) : 1719\n",
      " Accuracy : 85.95 \n"
     ]
    }
   ],
   "source": [
    "#Accuracy of Neural Network Using Last 2000 samples\n",
    "accuracy_act(X_test,updated_weights,updated_bias,activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_act           = 0.1          # Learning Rate\n",
    "iteration_act    = 1000       # No of Iterations\n",
    "activation       = \"relu\"     #Activattion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent With Moementum\n",
      "Activation Function : relu\n",
      "Epochs : 1000  Learning Rate : 0.1\n",
      "Weights and Bias are updating .....\n",
      "Final train Loss 2.300526039208449\n"
     ]
    }
   ],
   "source": [
    "#Initial Weight and Bias\n",
    "weights_act, bias_act = weight_bias_double_layer(in_layer,hid_layer_1,hid_layer_2,out_layer)\n",
    "\n",
    "#Updating Weights and Bias\n",
    "updated_weights,updated_bias = update_weights_double_layer_momentum(X, Y,weights_act,bias_act,lr_act, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                           ðŸ˜Š"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
